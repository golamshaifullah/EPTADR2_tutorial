{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPTA GWB analyses tutorial - adapted from material by Stas Babak and Siyuan Chen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/golamshaifullah/EPTADR2_tutorial/blob/main/tutorials/03_Enterprise_GWB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following two cells only when using colab! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will reset the kernel.\n",
    "# Run this cell, wait until it's done, then run the next.\n",
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install_mambaforge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!mamba install -y -c conda-forge enterprise_extensions la_forge corner \"scipy<1.13\"\n",
    "!git clone https://github.com/golamshaifullah/EPTADR2_tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual notebook starts from here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    homedir = '/content/EPTADR2_tutorial'\n",
    "else:\n",
    "    homedir = '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import os, glob, json\n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "\n",
    "import enterprise\n",
    "from enterprise.pulsar import Pulsar\n",
    "from enterprise.signals import utils\n",
    "from enterprise_extensions import models, model_utils, hypermodel\n",
    "from enterprise_extensions.sampler import JumpProposal\n",
    "\n",
    "from PTMCMCSampler.PTMCMCSampler import PTSampler as ptmcmc\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "options = SimpleNamespace()\n",
    "options.basedir =  f'{homedir}/data/EPTA_DR2'\n",
    "options.dataset = 'DR2new+'\n",
    "options.datadir = os.path.join(options.basedir, options.dataset)\n",
    "options.noisedir = os.path.join(options.basedir, 'noisefiles_t2equad', options.dataset)\n",
    "options.red_components =  0\n",
    "options.dm_components =  0\n",
    "options.chrom_components =  0\n",
    "options.common_components =  30\n",
    "options.common_psd =  'powerlaw'\n",
    "options.common_components =  30\n",
    "options.gamma_common =  None\n",
    "options.red_components =  0\n",
    "options.dm_components =  0\n",
    "options.chrom_components =  0\n",
    "options.num_dmdips =  2\n",
    "options.bayesephem =  False\n",
    "options.common_sin =  False\n",
    "options.psrname =  'string'\n",
    "options.resume =  False\n",
    "options.emp =  None\n",
    "options.number =  1e7\n",
    "options.thin =  100\n",
    "options.PsrList = ['J0613-0200','J1012+5307','J1600-3053','J1713+0747','J1744-1134','J1909-3744']\n",
    "options.orf_bins = None\n",
    "options.orf = 'crn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load par+tim+noise files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parfiles = sorted(glob.glob( options.datadir  + '/J*/*.par'))\n",
    "timfiles = sorted(glob.glob( options.datadir  + '/J*/*_all.tim'))\n",
    "noisefiles = sorted(glob.glob( options.noisedir  + '/*.json'))\n",
    "\n",
    "parfiles = [x for x in parfiles if x.split('/')[-1].split('.')[0] in  options.PsrList ]\n",
    "timfiles = [x for x in timfiles if x.split('/')[-1].split('_')[0] in  options.PsrList ]\n",
    "noisefiles = [x for x in noisefiles if x.split('/')[-1].split('_')[0] in  options.PsrList ]\n",
    "\n",
    "params = {}\n",
    "for nf in noisefiles:\n",
    "    with open(nf, 'r') as fin:\n",
    "        params.update(json.load(fin))\n",
    "\n",
    "def create_pulsar(parfile, timfile):\n",
    "    return Pulsar(parfile, timfile, ephem='DE440')\n",
    "\n",
    "# Create pulsar objects in parallel\n",
    "psrs = []\n",
    "\n",
    "if IN_COLAB:\n",
    "    for par, tim in zip(parfile, timfiles):\n",
    "        create_pulsar(par, tim)\n",
    "else:\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        # Use a dictionary to associate parfiles with timfiles\n",
    "        futures = {executor.submit(create_pulsar, p, t): (p, t) for p, t in zip(parfiles, timfiles)}\n",
    "        for future in futures:\n",
    "            psrs.append(future.result())\n",
    "\n",
    "# Check the number of pulsars created\n",
    "print(f\"Number of pulsar objects created: {len(psrs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load noise models and files\n",
    "params = {}\n",
    "for nf in noisefiles:\n",
    "    with open(nf, 'r') as fin:\n",
    "        params.update(json.load(fin))\n",
    "\n",
    "if not options.red_components:\n",
    "    try:\n",
    "        red_dict = {}\n",
    "        with open( options.noisedir  + '/red_dict.json','r') as rd:\n",
    "            red_dict.update(json.load(rd))\n",
    "    except:\n",
    "        raise UserWarning('Custom pulsar red noise frequency components not set.')\n",
    "else:\n",
    "    red_dict = options.red_components\n",
    "\n",
    "if not options.dm_components:\n",
    "    try:\n",
    "        dm_dict = {}\n",
    "        with open( options.noisedir  + '/dm_dict.json','r') as dd:\n",
    "            dm_dict.update(json.load(dd))\n",
    "    except:\n",
    "        raise UserWarning('Custom pulsar DM noise frequency components not set.')\n",
    "else:\n",
    "    dm_dict = options.dm_components\n",
    "\n",
    "if not options.chrom_components:\n",
    "    try:\n",
    "        chrom_dict = {}\n",
    "        with open( options.noisedir  + '/chrom_dict.json','r') as cd:\n",
    "            chrom_dict.update(json.load(cd))\n",
    "    except:\n",
    "        raise UserWarning('Custom pulsar scattering noise frequency components not set.')\n",
    "else:\n",
    "    chrom_dict = options.chrom_components\n",
    "\n",
    "try:\n",
    "    gamma_common = float(options.gamma_common)\n",
    "except:\n",
    "    gamma_common = None\n",
    "\n",
    "if options.psrname is not None:\n",
    "    dropout = True\n",
    "else:\n",
    "    dropout = False\n",
    "\n",
    "if options.orf_bins is not None:\n",
    "    orf_bins = np.loadtxt(options.orf_bins)\n",
    "else:\n",
    "    orf_bins = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming purely GW emission driven circular binaries allows one to write the strain of the GWB to be\n",
    "\\begin{equation}\n",
    "\\large\n",
    "h_c (f) = A_{GWB} f^{-2/3}\n",
    "\\end{equation}\n",
    "\n",
    "The characteristic strain $h_c$ is connected to the induced correlated red noise between two pulsars $i$ and $j$ via the power spectral density (which is the Fourier transform of the common residuals $R_{ij}(t)$ between pulsars $i$ and $j$)\n",
    "\\begin{equation}\n",
    "\\large\n",
    "S_{ij}(f) = \\Gamma_{ij} \\frac{h_c^2(f)}{12\\pi^2 f^3}\n",
    "\\end{equation}\n",
    "where $\\Gamma_{ij}$ is the overlap reduction function and describes the degree of correlation between the noise in the pulsar pair $ij$, in case of an isotropic GWB it is the Hellings-Downs curve.\n",
    "\n",
    "We can put $h_c$ into the PSD equation to get\n",
    "\\begin{equation}\n",
    "\\large\n",
    "S_{ij}(f) = \\Gamma_{ij} \\frac{A_{GWB}^2 f^{-4/3}}{12\\pi^2 f^3} = \\frac{\\Gamma_{ij}}{12\\pi^2} A_{GWB}^2 f^{-13/3}\n",
    "\\end{equation}\n",
    "\n",
    "White noise parameters are used fixed for the GWB analysis. See Gregory+Antoine tutorial on how to get EFAC+EQUAD.\n",
    "\n",
    "To speed computation we usually assume that the overlap reduction function is just the identity matrix, ie. common red noise process with no spatial correlation. But HD correlated red noise search is done as a final confirmation.\n",
    "\n",
    "When searching for a background, sometimes the $-\\gamma=-13/3$ restriction is loosened to be $\\gamma \\in [0,7]$. This is equivalent to searching for a common red noise amongst all pulsars with a unknown spectral index and amplitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GWB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create PTA object\n",
    "pta = models.model_general(psrs, noisedict=params, orf=options.orf, \n",
    "                           gamma_common=13./3., upper_limit_common=True, \n",
    "                           bayesephem=True, dm_var=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw initial sample\n",
    "x0 = np.hstack(p.sample() for p in pta.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PTMCMC\n",
    "\n",
    "# set output directory\n",
    "outdir = '../chains/ptmcmc_test'\n",
    "\n",
    "# save parameter names\n",
    "np.savetxt(outdir+'/pars.txt', pta.param_names, fmt='%s')\n",
    "\n",
    "ndim = len(x0)\n",
    "N = int(1e4)\n",
    "\n",
    "# initial jump covariance matrix\n",
    "cov = np.diag(np.ones(ndim) * 0.01**2)\n",
    "\n",
    "sampler = ptmcmc(ndim, pta.get_lnlikelihood, pta.get_lnprior, cov, outDir=outdir, resume=False)\n",
    "\n",
    "# jump proposals\n",
    "jp = JumpProposal(pta)\n",
    "sampler.addProposalToCycle(jp.draw_from_prior,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SCAM = Single Component Adaptive Metropolis\n",
    "# AM = Adaptive Metropolis\n",
    "# DE = Differential Evolution\n",
    "## You can keep all these set at default values\n",
    "sampler.sample(x0, N, SCAMweight=30, AMweight=15, DEweight=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = np.genfromtxt(f\"{outdir}/chain_1.txt\")\n",
    "names = np.loadtxt(f\"{outdir}/pars.txt\",dtype=str)\n",
    "chain = np.delete(chain,[chain.shape[1]-1,chain.shape[1]-2,chain.shape[1]-3,chain.shape[1]-4],1)\n",
    "burn = int(0.25 * chain.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot chain and posterior\n",
    "\n",
    "s = 7 # 7 ptmcmc, 1 single\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.title('{}'.format(names[-s]))\n",
    "plt.plot(range(len(chain)),chain[:,-s])\n",
    "\n",
    "plt.subplot(122)\n",
    "uplim = 10.**np.percentile(chain[burn:,-s],95)\n",
    "plt.hist(10.**chain[burn:,-s], 50, density=True, histtype='step', lw=2)\n",
    "plt.axvline(uplim,label=\"{:.2e}\".format(uplim))\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# corner plot\n",
    "corner.corner(chain[burn:,[-7,-6]],labels=[names[-7],names[-6]],show_titles=1,quantiles=[0.05,0.5,0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GWB Model comparison\n",
    "\n",
    "Here we use the `HyperModel` from `enterprise_extensions`. It creates a Bayesian hyper model with all parameters from the constituent models + a parameter `n_model` specifying which sub-model is being sampled. The fraction of steps that the sampler stays in model0 vs model1 give the odds-ratio between the two models.\n",
    "\n",
    "This is typically used to gauge whether a common red noise process has a notable Bayes factor to be Hellings-Downs correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Hypermodel to compute Bayes factors between different models\n",
    "pta = dict.fromkeys(np.arange(0, 2))\n",
    "\n",
    "pta[0] = models.model_general(psrs, noisedict=params,\n",
    "                              gamma_common=13./3., upper_limit_common=True,\n",
    "                              bayesephem=False, dm_var=True)\n",
    "\n",
    "pta[1] = models.model_general(psrs, noisedict=params, orf='hd', \n",
    "                              gamma_common=13./3., upper_limit_common=True,\n",
    "                              bayesephem=False, dm_var=True)\n",
    "\n",
    "super_model = hypermodel.HyperModel(pta)\n",
    "\n",
    "outdir = f'{homedir}/chains/hyper_model_test/'\n",
    "sampler = super_model.setup_sampler(resume=False, outdir=outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample hypermodel\n",
    "N = int(1e4)\n",
    "x0 = super_model.initial_sample()\n",
    "sampler.sample(x0, N, SCAMweight=30, AMweight=15, DEweight=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post processing\n",
    "chain = np.genfromtxt(f'{homedir}/chains/hyper_model_test/chain_1.txt')\n",
    "names = np.loadtxtf'{homedir}/chains/hyper_model_test/pars.txt',dtype=str)\n",
    "burn = int(0.25 * chain.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute odds ratio between model 0 and 1\n",
    "model_utils.odds_ratio(chain[burn:,-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corner plot\n",
    "corner.corner(chain[burn:,[-6,-5]],labels=[names[-2],names[-1]],show_titles=1,quantiles=[0.05,0.5,0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
